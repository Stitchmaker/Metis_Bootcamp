{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Analysis\n",
    "\n",
    "Use LDA to take dataset of documents and sort into buckets.\n",
    "\n",
    "Use K Means to group similar buckets into clusters.\n",
    "\n",
    "Plot the fraction of documents in a cluster over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:00:23.605233Z",
     "start_time": "2018-11-19T14:00:23.415712Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:57:08.822207Z",
     "start_time": "2018-11-19T13:57:08.473407Z"
    }
   },
   "outputs": [],
   "source": [
    "# read data from pickle file\n",
    "with open ('data/all_data_desk_nohole.pkl', 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:57:08.923278Z",
     "start_time": "2018-11-19T13:57:08.824031Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# look at how many articles come from each news desk to choose which to include\n",
    "foo = df.groupby('news_desk').count()\n",
    "foo = foo.sort_values(by=['date'],ascending=False)\n",
    "foo.iloc[61:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:57:09.570914Z",
     "start_time": "2018-11-19T13:57:09.056758Z"
    }
   },
   "outputs": [],
   "source": [
    "# there's some bad data in the snippets.  Drop the rows that have NaN to clean this up\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# choose which news desks to include\n",
    "news_desk_use = ['Business',\n",
    "                 'Foreign',\n",
    "                 'NewsDesk',\n",
    "                 'National',\n",
    "                 'Politics',\n",
    "                 'U.S.',\n",
    "                 'U.S. / Politics',\n",
    "                 'U.S. / Election 2016',\n",
    "                 'Washington',\n",
    "                 'World / Europe',\n",
    "                 'World / Middle East',\n",
    "                 'World / Asia Pacific',\n",
    "                 'World / Africa',\n",
    "                 'World / Americas']\n",
    "df2 = pd.DataFrame()\n",
    "for desk in news_desk_use:\n",
    "    topic = df['news_desk'] == desk\n",
    "    df2 = pd.concat([df2,df[topic]])\n",
    "    \n",
    "# pull a fraction of the dataset for testing\n",
    "#df2 = df2.sample(200)\n",
    "\n",
    "# create an new dataframe that combines the headline and snippet (more words in document)\n",
    "df2['head_snip'] = df2['headline'] + ' '+ df2['snippet']\n",
    "\n",
    "dates = list(df2['date'].copy())\n",
    "\n",
    "documents = list(df2['head_snip'].copy())\n",
    "\n",
    "df2.shape\n",
    "\n",
    "# write data to pickle file\n",
    "#with open('data/df2.pkl', 'wb') as fp:\n",
    "#    pickle.dump(df2, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T13:57:23.849875Z",
     "start_time": "2018-11-19T13:57:23.814384Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T22:01:43.904337Z",
     "start_time": "2018-11-14T22:01:43.417870Z"
    }
   },
   "outputs": [],
   "source": [
    "# write files to csv for transfer to AWS\n",
    "df2['date'].to_csv('data/date.csv')\n",
    "df2['head_snip'].to_csv('data/documents.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files from csv on AWS\n",
    "date_df = pd.read_csv('data/date.csv')\n",
    "documents_df = pd.read_csv('data/documents.csv')\n",
    "date = date_df.iloc[:,1].tolist()\n",
    "documents = documents_df.iloc[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T15:04:18.491341Z",
     "start_time": "2018-11-17T15:04:18.432649Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.groupby('news_desk').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:00:32.847533Z",
     "start_time": "2018-11-19T14:00:32.843393Z"
    }
   },
   "outputs": [],
   "source": [
    "# for LDA show tho topics with the highest probability for a bucket\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(f\"Topic {topic_idx}\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print('document with index',doc_index)\n",
    "            print(documents[doc_index])\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T15:04:46.270160Z",
     "start_time": "2018-11-17T15:04:46.099345Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the trends in LDA buckets over time\n",
    "# can choose month or week\n",
    "\n",
    "def plot_topic_trend_lda(H, W, documents, dates):\n",
    "    plt.figure(figsize=(17,8))\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    min_year, min_week, weekday = min_date.isocalendar()\n",
    "    max_year, max_week, weekday = max_date.isocalendar()\n",
    "\n",
    "    plot_data_all = pd.DataFrame(columns=['topic','date','num_docs'])\n",
    "    \n",
    "    print('set up dataframe for data', plot_data_all.info())\n",
    "\n",
    "    #timeframe = 'week'\n",
    "    timeframe = 'month'\n",
    "    if timeframe == 'week':\n",
    "        week_start = min_week\n",
    "        week_end = (max_year-min_year)*52+max_week + 1\n",
    "\n",
    "        w = week_end - week_start\n",
    "        \n",
    "        for topic in range(len(H)):\n",
    "            for week in range(week_start,week_end): \n",
    "                plot_data_all.at[w*topic+week,'topic'] = topic\n",
    "                plot_data_all.at[w*topic+week,'date'] = week\n",
    "        print('weeks in dataframe', week_start, week_end, w)\n",
    "\n",
    "    elif timeframe == 'month':\n",
    "        month_start = min_date.month\n",
    "        max_month = max_date.month\n",
    "        month_end = (max_year-min_year)*12+max_month + 1\n",
    "        \n",
    "        m = month_end - month_start\n",
    "        \n",
    "        for topic in range(len(H)):\n",
    "            for month in range(month_start,month_end): \n",
    "                plot_data_all.at[m*topic+month,'topic'] = topic\n",
    "                plot_data_all.at[m*topic+month,'date'] = month\n",
    "        print('months in dataframe', month_start, month_end, m)\n",
    "    \n",
    "    plot_data_all['num_docs'] = 0\n",
    "    plot_data_all = plot_data_all.astype('int')    \n",
    "    \n",
    "    #print(plot_data_all)\n",
    "    \n",
    "    for doc_idx in range(W.shape[0]):\n",
    "        try:\n",
    "            topic_idx = W[doc_idx].argmax()\n",
    "\n",
    "            date_key = dates[doc_idx]\n",
    "            #print('date_key',doc_idx,date_key)\n",
    "            if timeframe == 'week':\n",
    "                year, week, weekday = date_key.isocalendar()\n",
    "\n",
    "                if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "                    week += 52\n",
    "                elif year == 2017:\n",
    "                    week += 104\n",
    "                elif year == 2018:\n",
    "                    week += 156\n",
    "                #print(w*topic_idx+week)\n",
    "                plot_data_all.loc[w*topic_idx + week,'num_docs'] +=  1\n",
    "                \n",
    "            elif timeframe == 'month':\n",
    "                year = date_key.year\n",
    "                month = date_key.month\n",
    "                if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "                    month += 12\n",
    "                elif year == 2017:\n",
    "                    month += 24\n",
    "                elif year == 2018:\n",
    "                    month += 36\n",
    "                #print(w*topic_idx+week)\n",
    "                #print(month, year, topic_idx, m, m*topic_idx + month)\n",
    "\n",
    "                plot_data_all.loc[m*topic_idx + month,'num_docs'] +=  1\n",
    "                 \n",
    "        except:\n",
    "            topic_idx = W[doc_idx].argmax()\n",
    "            date_key = dates[doc_idx]\n",
    "            year, week, weekday = date_key.isocalendar()\n",
    "\n",
    "            print(year, week,'did not work')\n",
    "            \n",
    "    # Normalize the values (percent per time period)\n",
    "    month_doc_totals = plot_data_all.groupby('date').sum()['num_docs']\n",
    "    \n",
    "    for i in range(len(plot_data_all)):\n",
    "        d = plot_data_all.iloc[i,1]\n",
    "        if month_doc_totals[d] != 0:\n",
    "            plot_data_all.iloc[i,2] /= month_doc_totals[d]\n",
    "    \n",
    "    for topic in range(len(H)):\n",
    "        foo = plot_data_all[plot_data_all['topic'] == topic]\n",
    "        plt.figure()\n",
    "        plt.plot(foo['date'],foo['num_docs'],label=f'Topic {topic}')\n",
    "        plt.legend(loc='upper left');\n",
    "    return\n",
    "    #return plot_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:24:53.050953Z",
     "start_time": "2018-11-15T03:24:53.044853Z"
    }
   },
   "outputs": [],
   "source": [
    "# write plot data to file for import in to Tableau\n",
    "plot_data_all.to_csv('data/plot_data_all_month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T15:04:54.887509Z",
     "start_time": "2018-11-17T15:04:54.877176Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the trends in K Means clusters over time\n",
    "# can choose month or week\n",
    "\n",
    "def plot_topic_trend_ldaKM(groups, predict, dates):\n",
    "    # groups = number of groups (int)\n",
    "    # predict = array of group number\n",
    "    # dates = array of dates that matches predict\n",
    "    plt.figure(figsize=(17,8))\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    min_year, min_week, weekday = min_date.isocalendar()\n",
    "    max_year, max_week, weekday = max_date.isocalendar()\n",
    "\n",
    "    plot_data_all = pd.DataFrame(columns=['topic','date','num_docs'])\n",
    "    \n",
    "    #timeframe = 'week'\n",
    "    timeframe = 'month'\n",
    "    if timeframe == 'week':\n",
    "        week_start = min_week\n",
    "        week_end = (max_year-min_year)*52+max_week + 1\n",
    "\n",
    "        w = week_end - week_start\n",
    "        \n",
    "        for topic in range(groups):\n",
    "            for week in range(week_start,week_end): \n",
    "                plot_data_all.at[w*topic+week,'topic'] = topic\n",
    "                plot_data_all.at[w*topic+week,'date'] = week\n",
    "        print('weeks in dataframe', week_start, week_end, w)\n",
    "\n",
    "    \n",
    "    elif timeframe == 'month':\n",
    "        month_start = min_date.month\n",
    "        max_month = max_date.month\n",
    "        month_end = (max_year-min_year)*12+max_month + 1\n",
    "        \n",
    "        m = month_end - month_start\n",
    "        \n",
    "        for topic in range(groups):\n",
    "            for month in range(month_start,month_end): \n",
    "                plot_data_all.at[m*topic+month,'topic'] = topic\n",
    "                plot_data_all.at[m*topic+month,'date'] = month\n",
    "        print('months in dataframe', month_start, month_end, m)\n",
    "    \n",
    "    plot_data_all['num_docs'] = 0\n",
    "    plot_data_all = plot_data_all.astype('int')    \n",
    "    print('set up dataframe for data',plot_data_all.info())\n",
    "\n",
    "    #print(plot_data_all)\n",
    "    \n",
    "    for doc_idx in range(predict.shape[0]):\n",
    "        try:\n",
    "            topic_idx = predict[doc_idx]\n",
    "\n",
    "            date_key = dates[doc_idx]\n",
    "            if timeframe == 'week':\n",
    "                year, week, weekday = date_key.isocalendar()\n",
    "\n",
    "                if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "                    week += 52\n",
    "                elif year == 2017:\n",
    "                    week += 104\n",
    "                elif year == 2018:\n",
    "                    week += 156\n",
    "                #print(w*topic_idx+week)\n",
    "                plot_data_all.loc[w*topic_idx + week,'num_docs'] +=  1\n",
    "                \n",
    "            elif timeframe == 'month':\n",
    "                year = date_key.year\n",
    "                month = date_key.month\n",
    "                #print('month, year', month, year)\n",
    "                if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "                    month += 12\n",
    "                elif year == 2017:\n",
    "                    month += 24\n",
    "                elif year == 2018:\n",
    "                    month += 36\n",
    "                #print(w*topic_idx+week)\n",
    "                #print(month, year, topic_idx, m, m*topic_idx + month)\n",
    "\n",
    "                plot_data_all.loc[m*topic_idx + month,'num_docs'] +=  1\n",
    "\n",
    "        except:\n",
    "            topic_idx = predict[doc_idx]\n",
    "            date_key = dates[doc_idx]\n",
    "            year, week, weekday = date_key.isocalendar()\n",
    "            month = date_key.month\n",
    "            #print(year, month,topic_idx, m*topic_idx + month,'did not work')\n",
    "        \n",
    "    # Normalize the values (percent per time period)\n",
    "    month_doc_totals = plot_data_all.groupby('date').sum()['num_docs']\n",
    "    \n",
    "    for i in range(len(plot_data_all)):\n",
    "        d = plot_data_all.iloc[i,1]\n",
    "        if month_doc_totals[d] != 0:\n",
    "            plot_data_all.iloc[i,2] /= month_doc_totals[d]\n",
    "\n",
    "    for topic in range(groups):\n",
    "        foo = plot_data_all[plot_data_all['topic'] == topic]\n",
    "        plt.figure()\n",
    "        plt.plot(foo['date'],foo['num_docs'],label=f'Topic {topic}')\n",
    "        plt.legend(loc='upper left');\n",
    "        \n",
    "#    return\n",
    "    return plot_data_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T15:05:21.349106Z",
     "start_time": "2018-11-17T15:05:06.018912Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "#tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "#tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_vectorizer = CountVectorizer(\n",
    "            max_df = 0.95, # leave out words that occur in more than 95% of docs\n",
    "            min_df = 2,    # leave out if occurs less than twice\n",
    "            ngram_range=(1,3),\n",
    "            stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:33:25.147905Z",
     "start_time": "2018-11-15T03:33:25.145562Z"
    }
   },
   "outputs": [],
   "source": [
    "#print(len(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-18T03:53:52.522155Z",
     "start_time": "2018-11-17T15:13:15.848131Z"
    }
   },
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=15, n_jobs=-1, learning_method='online', learning_offset=10.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## distances in LDA output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T21:40:22.112313Z",
     "start_time": "2018-11-13T21:40:22.082265Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "mat = lda_W\n",
    "\n",
    "print(0, documents[0])\n",
    "print(' ')\n",
    "\n",
    "print('TESTING WITH COSINE SIMILARITY:')\n",
    "dist = cosine_similarity(mat[0:1], mat)\n",
    "print('cosine similarity = ',max(dist[0][1:]))\n",
    "max_sim_index = np.argmax(dist[0][1:])\n",
    "print(max_sim_index, documents[max_sim_index])\n",
    "print(' ')\n",
    "for i in range(len(dist[0])):\n",
    "    if dist[0][i] >= 0.99:\n",
    "        print(dist[0][i],i)\n",
    "        print(documents[i])\n",
    "        print(' ')\n",
    "\n",
    "print('TESTING WITH EUCLIDEAN DISTANCE:')\n",
    "dist = euclidean_distances(mat[0:1], mat)\n",
    "print('euclidean distance = ',min(dist[0][1:]))\n",
    "min_sim_index = np.argmin(dist[0][1:])\n",
    "print(max_sim_index, documents[min_sim_index])\n",
    "for i in range(len(dist[0])):\n",
    "    if dist[0][i] <= 0.1:\n",
    "        print(dist[0][i],i)\n",
    "        print(documents[i])\n",
    "        print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T02:05:59.977437Z",
     "start_time": "2018-11-15T02:05:24.500879Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for c in range(1,51):\n",
    "    print(f'trying {c} clusters')\n",
    "    km = KMeans(n_clusters=c,random_state=10,n_init=1)\n",
    "    km.fit(lda_W)\n",
    "    output[c] = km.inertia_\n",
    "\n",
    "plt.plot(output.keys(),output.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T02:06:51.383418Z",
     "start_time": "2018-11-15T02:06:51.119853Z"
    }
   },
   "outputs": [],
   "source": [
    "c = 15\n",
    "km = KMeans(n_clusters=c,random_state=10,n_init=1)\n",
    "km.fit(lda_W)\n",
    "predict = km.predict(lda_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-12T23:20:54.269446Z",
     "start_time": "2018-11-12T23:20:54.265888Z"
    }
   },
   "outputs": [],
   "source": [
    "len(km.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T02:07:06.174057Z",
     "start_time": "2018-11-15T02:06:56.816144Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the trend over time for each cluster\n",
    "plot_topic_trend_ldaKM(c, predict, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T04:08:34.974978Z",
     "start_time": "2018-11-14T04:08:34.855205Z"
    }
   },
   "outputs": [],
   "source": [
    "# print articles closest to centroid from K-Means\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "centers = np.array(km.cluster_centers_)\n",
    "\n",
    "mat = lda_W\n",
    "\n",
    "for i in range(c):\n",
    "    print(f'K Means Cluster {i}')\n",
    "    cent = centers[i].reshape(50,-1)\n",
    "    cent = cent.T\n",
    "    #print(cent)\n",
    "    \n",
    "    dist = euclidean_distances(cent, mat)\n",
    "    min_dist = min(dist[0][1:])\n",
    "    #print('min euclidean distance = ',min_dist)\n",
    "    \n",
    "    dist_list = []\n",
    "    for pt in range(len(predict)):\n",
    "        if predict[pt] == i:\n",
    "            if dist[0][pt] < 1.3* min_dist:\n",
    "                dist_list.append([dist[0][pt],str(dates[pt].month)+'/'+str(dates[pt].year), documents[pt]])\n",
    "                #print(str(dates[pt].month)+'/'+str(dates[pt].year), documents[pt])\n",
    "    sort_dist_list = sorted(dist_list, key=lambda x: x[0])\n",
    "    for i in range(8):\n",
    "        try:\n",
    "            print(sort_dist_list[i][0], sort_dist_list[i][1],sort_dist_list[i][2])\n",
    "        except:\n",
    "            print('no more in list')\n",
    "    \n",
    "    print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
