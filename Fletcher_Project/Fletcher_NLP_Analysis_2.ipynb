{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T01:13:46.351087Z",
     "start_time": "2018-11-19T01:13:44.156420Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T01:14:20.903614Z",
     "start_time": "2018-11-19T01:14:20.410009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274606, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data from pickle file\n",
    "with open ('all_data_desk_nohole.pkl', 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "    \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T04:39:47.440481Z",
     "start_time": "2018-11-11T04:39:47.211918Z"
    }
   },
   "outputs": [],
   "source": [
    "# there's some bad data in the snippets.  Drop the rows that have NaN to clean this up\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# take a subset of dataset to try things out\n",
    "from sklearn.utils import shuffle\n",
    "df2 = df.sample(n=20)\n",
    "#df2 = shuffle(df2)\n",
    "\n",
    "#df2 = df.copy()\n",
    "\n",
    "# create an new dataframe that combines the headline and snippet (more words in document)\n",
    "\n",
    "#df2['head_snip'] = df2['headline'].add(df2.snippet)\n",
    "df2['head_snip'] = df['headline'] + ' '+ df['snippet']\n",
    "\n",
    "dates = list(df2['date'])\n",
    "\n",
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T04:39:53.253360Z",
     "start_time": "2018-11-11T04:39:53.250408Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = list(df2['head_snip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T04:39:53.593185Z",
     "start_time": "2018-11-11T04:39:53.587909Z"
    }
   },
   "outputs": [],
   "source": [
    "df2['head_snip'].to_csv('documents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T23:44:32.800476Z",
     "start_time": "2018-11-10T23:44:32.795799Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(f\"Topic {topic_idx} (with {len(np.unique(W[:,topic_idx]))} articles)\")\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print('document with index',doc_index)\n",
    "            print(documents[doc_index])\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_trend_lda_old(H, W, documents, dates):\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plot_data_all = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "    #for topic_idx in range(0,1):\n",
    "        count = 0\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        plot_data = {}\n",
    "        foo = W[:,topic_idx]\n",
    "        new_array = foo[np.where(foo != 0.)]\n",
    "        print('documents in topic',len(np.unique(new_array)))\n",
    "        \n",
    "        for doc_index in np.argsort(new_array):\n",
    "            #print(new_array[doc_index])\n",
    "            if(new_array[doc_index] > 0.5):\n",
    "                try:\n",
    "                    #print(doc_index)\n",
    "                    date_key = dates[int(doc_index)]\n",
    "                    year, week, weekday = date_key.isocalendar()\n",
    "                    if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "                        week += 52\n",
    "                    elif year == 2017:\n",
    "                        week += 104\n",
    "                    elif year == 2018:\n",
    "                        week += 156\n",
    "\n",
    "                    if week in plot_data:\n",
    "                        plot_data[week] += 1\n",
    "                    else:\n",
    "                        plot_data[week] = 1\n",
    "                except:\n",
    "                    print(doc_index, 'in', topic_idx, 'had a problem')\n",
    "\n",
    "        plt.plot(*zip(*sorted(plot_data.items())),label=f'Topic {topic_idx}')\n",
    "        plot_data_all.append(plot_data)\n",
    "    plt.legend(loc='upper left');\n",
    "    return\n",
    "#    return plot_data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T05:23:06.673708Z",
     "start_time": "2018-11-11T05:23:06.670568Z"
    }
   },
   "outputs": [],
   "source": [
    "len(lda_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T05:08:42.547034Z",
     "start_time": "2018-11-11T05:08:42.438424Z"
    }
   },
   "outputs": [],
   "source": [
    "min_date = min(dates)\n",
    "max_date = max(dates)\n",
    "min_year, min_week, weekday = min_date.isocalendar()\n",
    "max_year, max_week, weekday = max_date.isocalendar()\n",
    "max_year = 2015\n",
    "\n",
    "plot_data_all = pd.DataFrame(columns=[['topic','date','num_docs']])\n",
    "#plot_data_all['date'] = range(min_week,(max_year-min_year)*52+max_week+1)\n",
    "w = 10\n",
    "for topic in range(5):\n",
    "    for week in range(10): \n",
    "        plot_data_all.at[w*topic+week,'topic'] = topic\n",
    "        plot_data_all.at[w*topic+week,'date'] = week\n",
    "        \n",
    "plot_data_all['num_docs'] = 0\n",
    "plot_data_all = plot_data_all.astype('int')\n",
    "plot_data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T05:34:24.744789Z",
     "start_time": "2018-11-11T05:34:24.741455Z"
    }
   },
   "outputs": [],
   "source": [
    "len(lda_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:56:34.341364Z",
     "start_time": "2018-11-11T20:56:32.844137Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_topic_trend_lda(lda_H,lda_W,documents,dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T20:57:32.183782Z",
     "start_time": "2018-11-11T20:57:32.173173Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_topic_trend_lda(H, W, documents, dates):\n",
    "    plt.figure(figsize=(17,8))\n",
    "    min_date = min(dates)\n",
    "    max_date = max(dates)\n",
    "    min_year, min_week, weekday = min_date.isocalendar()\n",
    "    max_year, max_week, weekday = max_date.isocalendar()\n",
    "\n",
    "    plot_data_all = pd.DataFrame(columns=[['topic','date','num_docs']])\n",
    "    week_start = min_week\n",
    "    week_end = (max_year-min_year)*52+max_week\n",
    "\n",
    "    w = week_end - week_start\n",
    "    for topic in range(len(H)):\n",
    "        for week in range(week_start,week_end): \n",
    "            #print(w*topic+week, topic, week)\n",
    "            plot_data_all.at[w*topic+week,'topic'] = topic\n",
    "            plot_data_all.at[w*topic+week,'date'] = week\n",
    "    plot_data_all['num_docs'] = 0\n",
    "    plot_data_all = plot_data_all.astype('int')    \n",
    "    \n",
    "    #print(plot_data_all)\n",
    "\n",
    "    for doc_idx in range(W.shape[0]):\n",
    "        topic_idx = W[doc_idx].argmax()\n",
    "        #print('doc_idx,topic_idx',doc_idx,topic_idx)\n",
    "        #print('dates',dates[doc_idx])\n",
    "\n",
    "        date_key = dates[doc_idx]\n",
    "        #print('date key',date_key)\n",
    "        year, week, weekday = date_key.isocalendar()\n",
    "        #print('week',week)\n",
    "        if year == 2016:   # 2015 gets weeks 1-52, subsequent years have to add multiples of 52 to week number\n",
    "            week += 52\n",
    "        elif year == 2017:\n",
    "            week += 104\n",
    "        elif year == 2018:\n",
    "            week += 156\n",
    "        #print('plot_data_all index',w*topic_idx+week, week)\n",
    "        \n",
    "        #bar = plot_data_all.at[w*topic_idx + week,'num_docs']\n",
    "        #print('bar',bar)\n",
    "        #plot_data_all.at[w*topic_idx + week,'num_docs'] = bar + 1\n",
    "        bar = w*topic_idx + week\n",
    "        plot_data_all.loc[bar,'num_docs'] +=  1\n",
    "        \n",
    "    plot_data_all.plot(x='date', y='num_docs', style='o')\n",
    "\n",
    "  \n",
    "    #print(plot_data_all)\n",
    "#    plot_data = []\n",
    "#    for index, row in plot_data_all.iterrows():\n",
    "#        print([row['date'],row['num_docs']])\n",
    "#        plot_data.append = [row['date'],row['num_docs']]\n",
    "#    for topic in range(len(H)):\n",
    "#        foo = plot_data_all[plot_data_all['topic'] == topic]\n",
    "#        print(foo)\n",
    "#        plt.plot(foo['date'],foo['num_docs'])\n",
    "#    plt.legend(loc='upper left');\n",
    "#    plt.plot(plot_data_all['date'],['num_docs'])\n",
    "    return plot_data_all\n",
    "#    return plot_data_all\n",
    "\n",
    "def plot_topic_trend_nmf(H, W, documents, dates):\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plot_data_all = []\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "    #for topic_idx in range(0,1):\n",
    "        count = 0\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        plot_data = {}\n",
    "        foo = W[:,topic_idx]\n",
    "        new_array = foo[np.where(foo != 0.)]\n",
    "        print('documents in topic',len(np.unique(new_array)))\n",
    "\n",
    "        for doc_index in np.argsort(new_array):\n",
    "            try:\n",
    "                #print(doc_index)\n",
    "                date_key = dates[int(doc_index)]\n",
    "                year, week, weekday = date_key.isocalendar()\n",
    "                if year == 2016:\n",
    "                    week += 52\n",
    "                elif year == 2017:\n",
    "                    week += 104\n",
    "                elif year == 2018:\n",
    "                    week += 156\n",
    "\n",
    "                if week in plot_data:\n",
    "                    plot_data[week] += 1\n",
    "                else:\n",
    "                    plot_data[week] = 1\n",
    "            except:\n",
    "                print(doc_index, 'in', topic_idx, 'had a problem')\n",
    "\n",
    "        #plt.plot(*zip(*sorted(plot_data.items())),Ãš)\n",
    "        plot_data_all.append(plot_data)\n",
    "    plt.legend(loc='upper left');\n",
    "    #return\n",
    "    return plot_data_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T23:52:56.214376Z",
     "start_time": "2018-11-10T23:52:53.695015Z"
    }
   },
   "outputs": [],
   "source": [
    "no_features = 2000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, \n",
    "                                   min_df=2, \n",
    "                                   max_features=no_features, \n",
    "                                   stop_words='english')\n",
    "#tfidf_vectorizer = TfidfVectorizer(**tf_vectorizer.get_params())\n",
    "tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T23:20:06.497170Z",
     "start_time": "2018-11-10T23:20:06.485494Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T23:53:24.004085Z",
     "start_time": "2018-11-10T23:53:12.162907Z"
    }
   },
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf_model = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "nmf_W = nmf_model.transform(tfidf)\n",
    "nmf_H = nmf_model.components_\n",
    "\n",
    "no_top_words = 10\n",
    "no_top_documents = 2\n",
    "display_topics(nmf_H, nmf_W, tfidf_feature_names, documents, no_top_words, no_top_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T01:42:24.704152Z",
     "start_time": "2018-11-11T01:42:24.067025Z"
    }
   },
   "outputs": [],
   "source": [
    "#plot_data_all = plot_topic_trend(nmf_H, nmf_W, tfidf_feature_names, documents, dates)\n",
    "plot_topic_trend_nmf(nmf_H, nmf_W, documents, dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:05:33.287862Z",
     "start_time": "2018-11-11T02:05:33.020184Z"
    }
   },
   "outputs": [],
   "source": [
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "#tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "#tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_vectorizer = CountVectorizer(\n",
    "            max_df = 0.95, # leave out words that occur in more than 95% of docs\n",
    "            min_df = 2,    # leave out if occurs less than twice\n",
    "            ngram_range=(1,3),\n",
    "            stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:05:33.625961Z",
     "start_time": "2018-11-11T02:05:33.622871Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(tf_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T05:33:33.403052Z",
     "start_time": "2018-11-11T05:33:33.365188Z"
    }
   },
   "outputs": [],
   "source": [
    "no_topics = 5\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.fit_transform(tf)\n",
    "lda_H = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T05:33:39.414953Z",
     "start_time": "2018-11-11T05:33:39.409001Z"
    }
   },
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "no_top_documents = 5\n",
    "display_topics(lda_H, lda_W, tf_feature_names, documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T20:22:59.080507Z",
     "start_time": "2018-11-10T20:22:58.828945Z"
    }
   },
   "outputs": [],
   "source": [
    "# write model to file (or read from file)\n",
    "\n",
    "write = True # True if write to file\n",
    "             # False if read from file\n",
    "if write == True:\n",
    "    # write data to pickle file\n",
    "    with open('lda_model.pkl', 'wb') as fp:\n",
    "        pickle.dump(lda_model, fp)\n",
    "    with open('nmf_model.pkl', 'wb') as fp:\n",
    "        pickle.dump(nmf_model, fp)\n",
    "\n",
    "elif write == False:\n",
    "    # read data from pickle file\n",
    "    with open('lda_model.pkl', 'rb') as fp:\n",
    "        lda_model = pickle.load(fp)\n",
    "    with open('nmf_model_df95.pkl', 'rb') as fp:\n",
    "        nmf_model = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T04:22:33.076816Z",
     "start_time": "2018-11-09T04:22:33.064335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "topic_idx = 4\n",
    "foo = nmf_W[:,topic_idx]\n",
    "new_array = foo[np.where(foo != 0.)]\n",
    "for index in np.argsort(new_array):    \n",
    "    print(index)\n",
    "print(len(new_array))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:05:46.832668Z",
     "start_time": "2018-11-11T02:05:46.828995Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in range(lda_W.shape[0]):\n",
    "    topic_most_pr = lda_W[n].argmax()\n",
    "    print(\"doc: {} topic: {}\\n\".format(n,topic_most_pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T02:06:01.846782Z",
     "start_time": "2018-11-11T02:06:01.668446Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_topic_trend_lda(lda_H, lda_W, documents, dates)\n",
    "#plot_data_all  = plot_topic_trend(lda_H, lda_W, documents, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-10T21:47:36.042877Z",
     "start_time": "2018-11-10T21:47:36.037296Z"
    }
   },
   "outputs": [],
   "source": [
    "# write data to csv file for reading into Tableau\n",
    "\n",
    "outfile = open( 'plot_data_all_lda.csv', 'w' )\n",
    "for i in range(len(plot_data_all)):\n",
    "    for key, value in sorted( plot_data_all[:][i].items() ):\n",
    "        outfile.write( str(i) + ',' + str(key) + ',' + str(value) + '\\n' )\n",
    "#        outfile.write( str(i) + '\\t' + str(key) + '\\t' + str(value) + '\\n' )\n",
    "#        print( str(i) + '\\t' + str(key) + '\\t' + str(value) + '\\n' )\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-11T00:37:19.145303Z",
     "start_time": "2018-11-11T00:36:22.779731Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "pyLDAvis.sklearn.prepare(lda_model, tf, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:25:12.642209Z",
     "start_time": "2018-11-08T22:25:12.639433Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup nltk corpora path and Google Word2Vec location\n",
    "google_vec_file = '/Users/dana/metis/GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:30:43.917517Z",
     "start_time": "2018-11-08T22:30:09.488902Z"
    }
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(google_vec_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:50:25.778036Z",
     "start_time": "2018-11-08T22:50:25.773996Z"
    }
   },
   "outputs": [],
   "source": [
    "headline = df2['headline'][0:10]\n",
    "print(headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:47:24.461115Z",
     "start_time": "2018-11-08T22:47:24.457883Z"
    }
   },
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:32:56.435302Z",
     "start_time": "2018-11-08T22:32:56.408185Z"
    }
   },
   "outputs": [],
   "source": [
    "headline = df2['headline'][0:10]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for index, row in headline.iteritems():\n",
    "    word_tokens = word_tokenize(row)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    stemmed_sentence = [ps.stem(w) for w in filtered_sentence]\n",
    "    print(stemmed_sentence)#model.word_vec('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:50:58.794084Z",
     "start_time": "2018-11-08T22:50:58.786035Z"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in headline.iteritems():\n",
    "    word_tokens = word_tokenize(row)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    #stemmed_sentence = [ps.stem(w) for w in filtered_sentence]\n",
    "    stemmed_sentence = filtered_sentence\n",
    "    print(stemmed_sentence)#model.word_vec('word')\n",
    "    \n",
    "    headline_vec = []\n",
    "\n",
    "    for word in stemmed_sentence:\n",
    "        try:\n",
    "            vec = model.word_vec(word)\n",
    "            headline_vec.append(vec)\n",
    "\n",
    "        except:\n",
    "            print('*not found*   ', word)\n",
    "\n",
    "    print(len(headline_vec))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-08T22:50:02.541236Z",
     "start_time": "2018-11-08T22:50:02.538045Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(headline_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
